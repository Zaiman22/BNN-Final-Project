{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST and CIFAR-10 With BNN\n",
    "\n",
    "one of the thing you can make as a newbie with NN is a MNIST clasification. This file is praticly the same as the [Introduction to BNNs with Larq](https://docs.larq.dev/larq/tutorials/mnist/) with extra comment and gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environement\n",
    "\n",
    "The LARQ library only compatitble with a certain version of tensorflow, hence also the cuda, so do keep in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n",
      "Larq version: 0.13.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "import larq as lq\n",
    "print(\"Larq version:\", lq.__version__)\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 05:04:00.235376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-16 05:04:00.259770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-16 05:04:00.260085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "print(\"Is CUDA available:\", tf.test.is_built_with_cuda())\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ImageNet-like dataset (e.g., CIFAR-10 as a smaller example)\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1\n",
    "\n",
    "# Convert labels to one-dimensional arrays\n",
    "train_labels = train_labels.flatten()\n",
    "test_labels = test_labels.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaim22/anaconda3/envs/larq-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-16 05:04:02.525480: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-16 05:04:02.527170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-16 05:04:02.527460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-16 05:04:02.527658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-16 05:04:03.311985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-16 05:04:03.312282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-16 05:04:03.312479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-16 05:04:03.312633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3380 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2025-06-16 05:04:05.341315: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1228800000 exceeds 10% of free system memory.\n",
      "2025-06-16 05:04:06.641711: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 245760000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"binary_resnet_e_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 64)   1728        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " quant_conv2d (QuantConv2D)     (None, 32, 32, 64)   36864       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 64)  256         ['quant_conv2d[0][0]']           \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 64)   0           ['batch_normalization[0][0]',    \n",
      "                                                                  'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " quant_conv2d_1 (QuantConv2D)   (None, 32, 32, 64)   36864       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 64)  256         ['quant_conv2d_1[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 64)   0           ['batch_normalization_1[0][0]',  \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " quant_conv2d_2 (QuantConv2D)   (None, 32, 32, 64)   36864       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 64)  256         ['quant_conv2d_2[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 64)   0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " quant_conv2d_3 (QuantConv2D)   (None, 32, 32, 64)   36864       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 64)  256         ['quant_conv2d_3[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 64)   0           ['batch_normalization_3[0][0]',  \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 32, 32, 64)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " quant_conv2d_5 (QuantConv2D)   (None, 16, 16, 128)  73728       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " quant_conv2d_4 (QuantConv2D)   (None, 16, 16, 128)  8192        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 128)  512        ['quant_conv2d_5[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 128)  512        ['quant_conv2d_4[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 16, 16, 128)  0           ['batch_normalization_5[0][0]',  \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " quant_conv2d_6 (QuantConv2D)   (None, 16, 16, 128)  147456      ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 16, 16, 128)  512        ['quant_conv2d_6[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 16, 128)  0           ['batch_normalization_6[0][0]',  \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " quant_conv2d_7 (QuantConv2D)   (None, 16, 16, 128)  147456      ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 16, 128)  512        ['quant_conv2d_7[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 16, 16, 128)  0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " quant_conv2d_8 (QuantConv2D)   (None, 16, 16, 128)  147456      ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 128)  512        ['quant_conv2d_8[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 16, 128)  0           ['batch_normalization_8[0][0]',  \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 128)  0          ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " quant_conv2d_10 (QuantConv2D)  (None, 8, 8, 256)    294912      ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " quant_conv2d_9 (QuantConv2D)   (None, 8, 8, 256)    32768       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 8, 8, 256)   1024        ['quant_conv2d_10[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 8, 8, 256)   1024        ['quant_conv2d_9[0][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_10[0][0]', \n",
      "                                                                  'batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " quant_conv2d_11 (QuantConv2D)  (None, 8, 8, 256)    589824      ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 8, 8, 256)   1024        ['quant_conv2d_11[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_11[0][0]', \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " quant_conv2d_12 (QuantConv2D)  (None, 8, 8, 256)    589824      ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 8, 8, 256)   1024        ['quant_conv2d_12[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 8, 8, 256)    0           ['batch_normalization_12[0][0]', \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " quant_conv2d_13 (QuantConv2D)  (None, 8, 8, 256)    589824      ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 8, 8, 256)   1024        ['quant_conv2d_13[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 8, 8, 256)    0           ['batch_normalization_13[0][0]', \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " quant_conv2d_15 (QuantConv2D)  (None, 4, 4, 512)    1179648     ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " quant_conv2d_14 (QuantConv2D)  (None, 4, 4, 512)    131072      ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 4, 4, 512)   2048        ['quant_conv2d_15[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 4, 4, 512)   2048        ['quant_conv2d_14[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4, 4, 512)    0           ['batch_normalization_15[0][0]', \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " quant_conv2d_16 (QuantConv2D)  (None, 4, 4, 512)    2359296     ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 4, 4, 512)   2048        ['quant_conv2d_16[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4, 4, 512)    0           ['batch_normalization_16[0][0]', \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " quant_conv2d_17 (QuantConv2D)  (None, 4, 4, 512)    2359296     ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 4, 4, 512)   2048        ['quant_conv2d_17[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4, 4, 512)    0           ['batch_normalization_17[0][0]', \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " quant_conv2d_18 (QuantConv2D)  (None, 4, 4, 512)    2359296     ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 4, 4, 512)   2048        ['quant_conv2d_18[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4, 4, 512)    0           ['batch_normalization_18[0][0]', \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 4, 4, 512)    0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 512)   0           ['activation[0][0]']             \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 512)          0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           5130        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 10)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,183,306\n",
      "Trainable params: 11,173,834\n",
      "Non-trainable params: 9,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from larq_zoo.literature import BinaryResNetE18\n",
    "import tensorflow as tf\n",
    "\n",
    "# Resize and convert the dataset to match the expected input shape\n",
    "train_images_resized = tf.image.resize(train_images, (32, 32))\n",
    "test_images_resized = tf.image.resize(test_images, (32, 32))\n",
    "\n",
    "# Load the BinaryResNetE18 model\n",
    "model = BinaryResNetE18(input_shape=(32, 32, 3), include_top=True, weights=None, num_classes=10)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 05:04:42.025402: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 491520000 exceeds 10% of free system memory.\n",
      "2025-06-16 05:04:42.488887: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 491520000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 05:04:46.432323: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2025-06-16 05:04:48.275251: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.61GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-06-16 05:04:48.610914: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.61GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-06-16 05:04:48.743536: W tensorflow/core/common_runtime/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.96GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7/313 [..............................] - ETA: 1:35 - loss: 2.6759 - accuracy: 0.1641"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# model.load_weights(\"binary_resnet18_mnist.h5\")\n",
    "\n",
    "# Train the model\n",
    "with tf.device('/GPU:0'):  # Use '/GPU:0' for the first GPU or '/CPU:0' for CPU\n",
    "    start_time = time.time()\n",
    "    model.fit(train_images, train_labels, batch_size=128, epochs=10, validation_split=0.2, verbose=1)\n",
    "    end_time = time.time()\n",
    "    print('Total Training Time: %.2f min' % ((end_time - start_time) / 60))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 11s 34ms/step - loss: 1.4144 - accuracy: 0.6772\n",
      "Test Accuracy: 0.6772000193595886\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to binary_resnet18_mnist.h5\n"
     ]
    }
   ],
   "source": [
    "model.save(\"binary_resnet18_mnist.h5\")\n",
    "print(\"Model saved to binary_resnet18_mnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+binary_resnet_e_18 stats--------------------------------------------------------------------------------------+\n",
      "| Layer                   Input prec.            Outputs   # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n",
      "|                               (bit)                          x 1       x 1     (kB)                          |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| input_1                           -    (-1, 32, 32, 3)         0         0        0           ?            ? |\n",
      "| conv2d                            -   (-1, 32, 32, 64)         0      1728     6.75           0      1769472 |\n",
      "| quant_conv2d                      1   (-1, 32, 32, 64)     36864         0     4.50    37748736            0 |\n",
      "| batch_normalization               -   (-1, 32, 32, 64)         0       128     0.50           0            0 |\n",
      "| add                               -   (-1, 32, 32, 64)         0         0        0           ?            ? |\n",
      "| quant_conv2d_1                    1   (-1, 32, 32, 64)     36864         0     4.50    37748736            0 |\n",
      "| batch_normalization_1             -   (-1, 32, 32, 64)         0       128     0.50           0            0 |\n",
      "| add_1                             -   (-1, 32, 32, 64)         0         0        0           ?            ? |\n",
      "| quant_conv2d_2                    1   (-1, 32, 32, 64)     36864         0     4.50    37748736            0 |\n",
      "| batch_normalization_2             -   (-1, 32, 32, 64)         0       128     0.50           0            0 |\n",
      "| add_2                             -   (-1, 32, 32, 64)         0         0        0           ?            ? |\n",
      "| quant_conv2d_3                    1   (-1, 32, 32, 64)     36864         0     4.50    37748736            0 |\n",
      "| batch_normalization_3             -   (-1, 32, 32, 64)         0       128     0.50           0            0 |\n",
      "| add_3                             -   (-1, 32, 32, 64)         0         0        0           ?            ? |\n",
      "| quant_conv2d_5                    1  (-1, 16, 16, 128)     73728         0     9.00    18874368            0 |\n",
      "| quant_conv2d_4                    1  (-1, 16, 16, 128)      8192         0     1.00     2097152            0 |\n",
      "| batch_normalization_5             -  (-1, 16, 16, 128)         0       256     1.00           0            0 |\n",
      "| batch_normalization_4             -  (-1, 16, 16, 128)         0       256     1.00           0            0 |\n",
      "| add_4                             -  (-1, 16, 16, 128)         0         0        0           ?            ? |\n",
      "| quant_conv2d_6                    1  (-1, 16, 16, 128)    147456         0    18.00    37748736            0 |\n",
      "| batch_normalization_6             -  (-1, 16, 16, 128)         0       256     1.00           0            0 |\n",
      "| add_5                             -  (-1, 16, 16, 128)         0         0        0           ?            ? |\n",
      "| quant_conv2d_7                    1  (-1, 16, 16, 128)    147456         0    18.00    37748736            0 |\n",
      "| batch_normalization_7             -  (-1, 16, 16, 128)         0       256     1.00           0            0 |\n",
      "| add_6                             -  (-1, 16, 16, 128)         0         0        0           ?            ? |\n",
      "| quant_conv2d_8                    1  (-1, 16, 16, 128)    147456         0    18.00    37748736            0 |\n",
      "| batch_normalization_8             -  (-1, 16, 16, 128)         0       256     1.00           0            0 |\n",
      "| add_7                             -  (-1, 16, 16, 128)         0         0        0           ?            ? |\n",
      "| quant_conv2d_10                   1    (-1, 8, 8, 256)    294912         0    36.00    18874368            0 |\n",
      "| quant_conv2d_9                    1    (-1, 8, 8, 256)     32768         0     4.00     2097152            0 |\n",
      "| batch_normalization_10            -    (-1, 8, 8, 256)         0       512     2.00           0            0 |\n",
      "| batch_normalization_9             -    (-1, 8, 8, 256)         0       512     2.00           0            0 |\n",
      "| add_8                             -    (-1, 8, 8, 256)         0         0        0           ?            ? |\n",
      "| quant_conv2d_11                   1    (-1, 8, 8, 256)    589824         0    72.00    37748736            0 |\n",
      "| batch_normalization_11            -    (-1, 8, 8, 256)         0       512     2.00           0            0 |\n",
      "| add_9                             -    (-1, 8, 8, 256)         0         0        0           ?            ? |\n",
      "| quant_conv2d_12                   1    (-1, 8, 8, 256)    589824         0    72.00    37748736            0 |\n",
      "| batch_normalization_12            -    (-1, 8, 8, 256)         0       512     2.00           0            0 |\n",
      "| add_10                            -    (-1, 8, 8, 256)         0         0        0           ?            ? |\n",
      "| quant_conv2d_13                   1    (-1, 8, 8, 256)    589824         0    72.00    37748736            0 |\n",
      "| batch_normalization_13            -    (-1, 8, 8, 256)         0       512     2.00           0            0 |\n",
      "| add_11                            -    (-1, 8, 8, 256)         0         0        0           ?            ? |\n",
      "| quant_conv2d_15                   1    (-1, 4, 4, 512)   1179648         0   144.00    18874368            0 |\n",
      "| quant_conv2d_14                   1    (-1, 4, 4, 512)    131072         0    16.00     2097152            0 |\n",
      "| batch_normalization_15            -    (-1, 4, 4, 512)         0      1024     4.00           0            0 |\n",
      "| batch_normalization_14            -    (-1, 4, 4, 512)         0      1024     4.00           0            0 |\n",
      "| add_12                            -    (-1, 4, 4, 512)         0         0        0           ?            ? |\n",
      "| quant_conv2d_16                   1    (-1, 4, 4, 512)   2359296         0   288.00    37748736            0 |\n",
      "| batch_normalization_16            -    (-1, 4, 4, 512)         0      1024     4.00           0            0 |\n",
      "| add_13                            -    (-1, 4, 4, 512)         0         0        0           ?            ? |\n",
      "| quant_conv2d_17                   1    (-1, 4, 4, 512)   2359296         0   288.00    37748736            0 |\n",
      "| batch_normalization_17            -    (-1, 4, 4, 512)         0      1024     4.00           0            0 |\n",
      "| add_14                            -    (-1, 4, 4, 512)         0         0        0           ?            ? |\n",
      "| quant_conv2d_18                   1    (-1, 4, 4, 512)   2359296         0   288.00    37748736            0 |\n",
      "| batch_normalization_18            -    (-1, 4, 4, 512)         0      1024     4.00           0            0 |\n",
      "| add_15                            -    (-1, 4, 4, 512)         0         0        0           ?            ? |\n",
      "| activation                        -    (-1, 4, 4, 512)         0         0        0           ?            ? |\n",
      "| average_pooling2d                 -    (-1, 1, 1, 512)         0         0        0           0            0 |\n",
      "| flatten                           -          (-1, 512)         0         0        0           0            0 |\n",
      "| dense                             -           (-1, 10)         0      5130    20.04           0         5120 |\n",
      "| activation_1                      -           (-1, 10)         0         0        0           ?            ? |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "| Total                                                   11157504     16330  1425.79   553648128      1774592 |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "+binary_resnet_e_18 summary-------------------+\n",
      "| Total params                      11.2 M    |\n",
      "| Trainable params                  11.2 M    |\n",
      "| Non-trainable params              9.47 k    |\n",
      "| Model size                        1.39 MiB  |\n",
      "| Model size (8-bit FP weights)     1.35 MiB  |\n",
      "| Float-32 Equivalent               42.62 MiB |\n",
      "| Compression Ratio of Memory       0.03      |\n",
      "| Number of MACs                    555 M     |\n",
      "| Ratio of MACs that are binarized  0.9968    |\n",
      "+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "lq.models.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9137255..1.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select a random test image\n",
    "index = np.random.randint(0, len(test_images_resized))\n",
    "image = test_images_resized[index]\n",
    "true_label = test_labels[index]\n",
    "\n",
    "# Predict the label\n",
    "predictions = model.predict(image[np.newaxis, ...])\n",
    "predicted_label = np.argmax(predictions)\n",
    "\n",
    "# Display the image and prediction\n",
    "plt.imshow(tf.squeeze(image).numpy(), cmap='gray')\n",
    "plt.title(f\"True Label: {true_label}, Predicted: {predicted_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "larq-env",
   "language": "python",
   "name": "larq-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
